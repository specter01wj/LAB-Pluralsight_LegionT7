import os
import json
import utils
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
import sys

import openai
from openai import OpenAI
# Don't forget to set your OpenAI API key as the environment variable OPENAI_API_KEY
client = OpenAI()

# The model we're going to use, change it if you want
gpt_model = "gpt-3.5-turbo-1106"
# Price per input token for the model used (gpt-3.5-turbo-1106)
price_per_input_token = 0.001 / 1000
# Price per output token for the model used (gpt-3.5-turbo-1106)
price_per_output_token = 0.002 / 1000

@retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=2, exp_base=2, min=2, max=30),
        retry=retry_if_exception_type((openai.APIConnectionError, openai.APITimeoutError, openai.InternalServerError))
)
def chat_completions_request(messages, model=gpt_model, json_mode=True, tools=None, tool_choice="auto"):
    """
    Send a chat completion request to OpenAI's GPT model.
    
    Parameters
    ----------
    messages : list
        A list of message objects, where each object has a 'role' and 'content'.
    model : str, optional
        The ID of the model to use.
    json_mode : boolean, option
        Determines if JSON mode should be used.
    tools : list, optional
        A list of functions to use in the chat.
    tool_choice : str, optional
        Determines when the model should call a function.

    Returns
    -------
    str
        The message generated by the model in response.
    """

    api_params = {
        "model": model,  # Model ID.
        "messages": messages,  # List of message objects.
        "temperature": 0,  # This parameter controls the randomness of the model's output.
    }
    
    # To set JSON mode if needed
    if json_mode:
        api_params["response_format"] = { "type": "json_object" } 

    # If functions are passed, we need to add them to the API params
    if tools is not None:
        api_params["tools"] = tools  # List of functions.
        api_params["tool_choice"] = tool_choice  # When the model should call a function.

    # Send the API request and return the model's response.
    response = client.chat.completions.create(**api_params)
    # Print the number of tokens and the cost for the messages list
    utils.print_token_info(messages, model, response, price_per_input_token, price_per_output_token)
    
    return response.choices[0].message

@retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=2, exp_base=2, min=2, max=30),
        retry=retry_if_exception_type((openai.APIConnectionError, openai.APITimeoutError, openai.InternalServerError))
)
def has_moderation_issues(text):
    """
    Send a moderation request to OpenAI.

    Parameters
    ----------
        text (str): The text to moderate.
    """
    
    # Split the text into chunks
    chunks = utils.split_text_advanced(text)

    for chunk in chunks:
        # Send the API request and return the model's response.
        response = client.moderations.create(input=chunk)

        # Extract the value of flagged to see if the chunk violates OpenAI's content policy
        flagged = response.results[0].flagged
        if flagged:
            utils.format_moderation_response(response, chunk)
            return True
    
    return False

def process_transcript(transcript):
    """
    Process a conversation transcript, makes requests to a chat completion endpoint,
    and schedules a follow-up call based on the conversation.

    Parameters
    ----------
        transcript (str): The interview transcript to process.
    """
    
    if has_moderation_issues(transcript):
        sys.exit(1)
    else:
        print("Moderation passed")

    # Load initial system and user messages
    messages=[
        {"role": "system", "content": utils.open_file("system_prompt.txt")},
        {"role": "user", "content": utils.open_file("user_prompt_01.txt") + transcript}
    ]
    
    # Request chat completion after initial messages
    first_response =  chat_completions_request(messages)

    # Print a message to inform user the information has been extracted
    print("Information extracted...")

    # Append the first response to the messages list
    messages.append(first_response)

    # Convert the first response's content from JSON string to a Python dictionary
    info_object = json.loads(first_response.content.strip())

    # Add another user message to the messages list with the second prompt
    messages.append({"role": "user", "content": utils.open_file("user_prompt_02.txt")})

    # Request another chat completion after adding the user message
    second_response = chat_completions_request(messages)

    # Print a message to inform user the information has been analyzed
    print("Information analyzed...")

    # Append the second response to the messages list
    messages.append(second_response)

    # Convert the second response's content from JSON string to a Python dictionary
    json_to_append = json.loads(second_response.content.strip())

    # Append the second JSON object to the first JSON object
    info_object.update(json_to_append)

    # Save the final JSON object to a file
    utils.save_file(
        json.dumps(info_object, indent=4), 
        f'output/{info_object["candidate"]}-{info_object["datetime"]}.json'
    )
    # Print a message to inform user the information is saved
    print("Information saved...")

    # Add a user message requesting the scheduling of a follow-up call
    messages.append({"role": "user", "content": "Please schedule a follow-up call using the interview date extracted from the transcript."})
    
    # Request a chat completion with the intent of scheduling a follow-up call
    third_response = chat_completions_request(messages, tools=utils.get_follow_up_function_desc())
    
    # Append the third response to the messages list
    messages.append(third_response)
    
    # Extract the field tool_calls into a variable
    tool_calls = third_response.tool_calls

    # Check if a function was called in the third message
    if tool_calls:
        for tool_call in tool_calls:
            function_message = tool_call.function

            if function_message.name == "schedule_follow_up":
                # Convert the function arguments from JSON string to a Python dictionary
                args = json.loads(function_message.arguments)
                
                # Call the function with the parsed arguments
                function_response = utils.schedule_follow_up(
                    interviewer=args.get("interviewer"),
                    candidate=args.get("candidate"),
                    interview_date=args.get("interview_date"),
                    sentiment=args.get("sentiment")
                )

                # Add the function call and its response to the messages list
                messages.append({
                    "tool_call_id": tool_call.id,
                    "role": "tool",
                    "name": function_message.name,
                    "content": function_response
                })
                
                # Request a final chat completion after scheduling the follow-up call
                fourth_response = chat_completions_request(messages, json_mode=False)
                messages.append(fourth_response)
    else:
        # Print a message if no function was called
        print("No function was called.")
    
    # Print the conversation in a readable format
    utils.pretty_print_conversation(messages)

# Main program 
if __name__ == "__main__":
    # Directory where the transcripts are stored
    directory_path = "transcripts"

    # List all files in directory
    files = os.listdir(directory_path)

    # Loop through each file
    for file in files:
        # Check if the file extension is '.txt'
        if file.endswith('.txt'):
            # Construct the full file path
            file_path = os.path.join(directory_path, file)

            # Reads the file and process the transcript
            process_transcript(utils.open_file(file_path))
